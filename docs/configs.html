<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Configuration properties &#8212; Velox  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/nature.css?v=0f882399" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Monitoring" href="monitoring.html" />
    <link rel="prev" title="Delta Lake Functions" href="functions/delta/functions.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="monitoring.html" title="Monitoring"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="functions/delta/functions.html" title="Delta Lake Functions"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Velox  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Configuration properties</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="configuration-properties">
<h1>Configuration properties<a class="headerlink" href="#configuration-properties" title="Link to this heading">¶</a></h1>
<section id="generic-configuration">
<h2>Generic Configuration<a class="headerlink" href="#generic-configuration" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>preferred_output_batch_bytes</p></td>
<td><p>integer</p></td>
<td><p>10MB</p></td>
<td><p>Preferred size of batches in bytes to be returned by operators from Operator::getOutput. It is used when an
estimate of average row size is known. Otherwise preferred_output_batch_rows is used.</p></td>
</tr>
<tr class="row-odd"><td><p>preferred_output_batch_rows</p></td>
<td><p>integer</p></td>
<td><p>1024</p></td>
<td><p>Preferred number of rows to be returned by operators from Operator::getOutput. It is used when an estimate of
average row size is not known. When the estimate of average row size is known, preferred_output_batch_bytes is used.</p></td>
</tr>
<tr class="row-even"><td><p>max_output_batch_rows</p></td>
<td><p>integer</p></td>
<td><p>10000</p></td>
<td><p>Max number of rows that could be return by operators from Operator::getOutput. It is used when an estimate of
average row size is known and preferred_output_batch_bytes is used to compute the number of output rows.</p></td>
</tr>
<tr class="row-odd"><td><p>merge_join_output_batch_start_size</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Initial output batch size in rows for MergeJoin operator. When non-zero, the batch size starts at this value
and is dynamically adjusted based on the average row size of previous output batches. When zero (default),
dynamic adjustment is disabled and the batch size is fixed at preferred_output_batch_rows.</p></td>
</tr>
<tr class="row-even"><td><p>max_elements_size_in_repeat_and_sequence</p></td>
<td><p>integer</p></td>
<td><p>10000</p></td>
<td><p>Max number of elements that can be set in <cite>repeat</cite> and <cite>sequence</cite> functions.</p></td>
</tr>
<tr class="row-odd"><td><p>table_scan_getoutput_time_limit_ms</p></td>
<td><p>integer</p></td>
<td><p>5000</p></td>
<td><p>TableScan operator will exit getOutput() method after this many milliseconds even if it has no data to return yet. Zero means ‘no time limit’.</p></td>
</tr>
<tr class="row-even"><td><p>abandon_partial_topn_row_number_min_rows</p></td>
<td><p>integer</p></td>
<td><p>100,000</p></td>
<td><p>Number of input rows to receive before starting to check whether to abandon partial TopNRowNumber.</p></td>
</tr>
<tr class="row-odd"><td><p>abandon_partial_topn_row_number_min_pct</p></td>
<td><p>integer</p></td>
<td><p>80</p></td>
<td><p>Abandons partial TopNRowNumber if number of output rows equals or exceeds this percentage of the number of input rows.</p></td>
</tr>
<tr class="row-even"><td><p>abandon_dedup_hashmap_min_rows</p></td>
<td><p>integer</p></td>
<td><p>100,000</p></td>
<td><p>Number of input rows to receive before starting to check whether to abandon building a HashTable without
duplicates in HashBuild for left semi/anti join.</p></td>
</tr>
<tr class="row-odd"><td><p>abandon_dedup_hashmap_min_pct</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Abandons building a HashTable without duplicates in HashBuild for left semi/anti join if the percentage of
distinct keys in the HashTable exceeds this threshold. Zero means ‘disable this optimization’.</p></td>
</tr>
<tr class="row-even"><td><p>session_timezone</p></td>
<td><p>string</p></td>
<td></td>
<td><p>User provided session timezone. Stores a string with the actual timezone name, e.g: “America/Los_Angeles”.</p></td>
</tr>
<tr class="row-odd"><td><p>adjust_timestamp_to_session_timezone</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, timezone-less timestamp conversions (e.g. string to timestamp, when the string does not specify a timezone)
will be adjusted to the user provided <cite>session_timezone</cite> (if any). For instance: if this option is true and user
supplied “America/Los_Angeles”, then “1970-01-01” will be converted to -28800 instead of 0. Similarly, timestamp
to date conversions will adhere to user ‘session_timezone’, e.g: Timestamp(0) to Date will be -1 (number of days
since epoch) for “America/Los_Angeles”.</p></td>
</tr>
<tr class="row-even"><td><p>track_operator_cpu_usage</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether to track CPU usage for stages of individual operators. Can be expensive when processing small batches,
e.g. &lt; 10K rows.</p></td>
</tr>
<tr class="row-odd"><td><p>operator_batch_size_stats_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, the driver will collect the operator’s input/output batch size through vector flat size estimation, otherwise not.
We might turn this off in use cases which have very wide column width and batch size estimation has non-trivial cpu cost.</p></td>
</tr>
<tr class="row-even"><td><p>hash_adaptivity_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If false, the ‘group by’ code is forced to use generic hash mode hashtable.</p></td>
</tr>
<tr class="row-odd"><td><p>adaptive_filter_reordering_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, the conjunction expression can reorder inputs based on the time taken to calculate them.</p></td>
</tr>
<tr class="row-even"><td><p>parallel_join_build_rows_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, the hash probe drivers can output build-side rows in parallel for full and right joins (only when spilling is not
enabled by hash probe). If false, only the last prober is allowed to output build-side rows.</p></td>
</tr>
<tr class="row-odd"><td><p>max_local_exchange_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>32MB</p></td>
<td><p>Used for backpressure to block local exchange producers when the local exchange buffer reaches or exceeds this size.</p></td>
</tr>
<tr class="row-even"><td><p>max_local_exchange_partition_count</p></td>
<td><p>integer</p></td>
<td><p>2^32</p></td>
<td><p>Limits the number of partitions created by a local exchange. Partitioning data too granularly can lead to poor performance.
This setting allows increasing the task concurrency for all pipelines except the ones that require a local partitioning.
Affects the number of drivers for pipelines containing LocalPartitionNode and cannot exceed the maximum number of
pipeline drivers configured for the task.</p></td>
</tr>
<tr class="row-odd"><td><p>exchange.max_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>32MB</p></td>
<td><p>Size of buffer in the exchange client that holds data fetched from other nodes before it is processed.
A larger buffer can increase network throughput for larger clusters and thus decrease query processing time
at the expense of reducing the amount of memory available for other usage.</p></td>
</tr>
<tr class="row-even"><td><p>min_exchange_output_batch_bytes</p></td>
<td><p>integer</p></td>
<td><p>2MB</p></td>
<td><p>The minimum number of bytes to accumulate in the ExchangeQueue before unblocking a consumer. This is used to avoid
creating tiny batches which may have a negative impact on performance when the cost of creating vectors is high
(for example, when there are many columns). To avoid latency degradation, the exchange client unblocks a consumer
when 1% of the data size observed so far is accumulated.</p></td>
</tr>
<tr class="row-odd"><td><p>merge_exchange.max_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>128MB</p></td>
<td><p>The aggregate buffer size (in bytes) across all exchange clients generated by the merge exchange operator,
responsible for storing data retrieved from various nodes prior to processing. It is divided
equally among all clients and has an upper and lower limit of 32MB and 1MB, respectively, per
client. Enforced approximately, not strictly. A larger size can increase network throughput
for larger clusters and thus decrease query processing time at the expense of reducing the
amount of memory available for other usage.</p></td>
</tr>
<tr class="row-even"><td><p>skip_request_data_size_with_single_source_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, skip request data size if there is only single source.
This is used to optimize the Presto-on-Spark use case where each exchange client
has only one shuffle partition source.</p></td>
</tr>
<tr class="row-odd"><td><p>local_merge_source_queue_size</p></td>
<td><p>integer</p></td>
<td><p>2</p></td>
<td><p>Maximum number of vectors buffered in each local merge source before blocking to wait for consumers.</p></td>
</tr>
<tr class="row-even"><td><p>max_page_partitioning_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>32MB</p></td>
<td><p>The maximum size in bytes for the task’s buffered output when output is partitioned using hash of partitioning keys. See PartitionedOutputNode::Kind::kPartitioned.
The producer Drivers are blocked when the buffered size exceeds this.
The Drivers are resumed when the buffered size goes below OutputBufferManager::kContinuePct (90)% of this.</p></td>
</tr>
<tr class="row-odd"><td><p>partitioned_output_eager_flush</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, the PartitionedOutput operator will flush rows eagerly, without waiting until buffers reach certain size. Default is false.</p></td>
</tr>
<tr class="row-even"><td><p>max_output_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>32MB</p></td>
<td><p>The maximum size in bytes for the task’s buffered output.
The producer Drivers are blocked when the buffered size exceeds this.
The Drivers are resumed when the buffered size goes below OutputBufferManager::kContinuePct (90)% of this.</p></td>
</tr>
<tr class="row-odd"><td><p>min_table_rows_for_parallel_join_build</p></td>
<td><p>integer</p></td>
<td><p>1000</p></td>
<td><p>The minimum number of table rows that can trigger the parallel hash join table build.</p></td>
</tr>
<tr class="row-even"><td><p>hash_probe_dynamic_filter_pushdown_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether hash probe can generate any dynamic filter (including Bloom filter) and push down to upstream operators.</p></td>
</tr>
<tr class="row-odd"><td><p>hash_probe_bloom_filter_pushdown_max_size</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>The maximum byte size of Bloom filter that can be generated from hash
probe.  When set to 0, no Bloom filter will be generated.  To achieve
optimal performance, this should not be too larger than the CPU cache
size on the host.</p></td>
</tr>
<tr class="row-even"><td><p>debug.validate_output_from_operators</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If set to true, then during execution of tasks, the output vectors of every operator are validated for consistency.
This is an expensive check so should only be used for debugging. It can help debug issues where malformed vector
cause failures or crashes by helping identify which operator is generating them.</p></td>
</tr>
<tr class="row-odd"><td><p>enable_expression_evaluation_cache</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether to enable caches in expression evaluation. If set to true, optimizations including vector pools and
evalWithMemo are enabled.</p></td>
</tr>
<tr class="row-even"><td><p>max_shared_subexpr_results_cached</p></td>
<td><p>integer</p></td>
<td><p>10</p></td>
<td><p>For a given shared subexpression, the maximum distinct sets of inputs we cache results for. Lambdas can call
the same expression with different inputs many times, causing the results we cache to explode in size. Putting
a limit contains the memory usage.</p></td>
</tr>
<tr class="row-odd"><td><p>driver_cpu_time_slice_limit_ms</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>If it is not zero, specifies the time limit that a driver can continuously
run on a thread before yield. If it is zero, then it no limit.</p></td>
</tr>
<tr class="row-even"><td><p>window_num_sub_partitions</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>Window operator can be configured to sub-divide window partitions on each thread of execution into groups of
sub partitions for sequential processing. This setting specifies how many sub-partitions to create for each
thread. Use 1 to disable sub partitioning.</p></td>
</tr>
<tr class="row-odd"><td><p>prefixsort_normalized_key_max_bytes</p></td>
<td><p>integer</p></td>
<td><p>128</p></td>
<td><p>Maximum number of bytes to use for the normalized key in prefix-sort. Use 0 to disable prefix-sort.</p></td>
</tr>
<tr class="row-even"><td><p>prefixsort_min_rows</p></td>
<td><p>integer</p></td>
<td><p>128</p></td>
<td><p>Minimum number of rows to use prefix-sort. The default value has been derived using micro-benchmarking.</p></td>
</tr>
<tr class="row-odd"><td><p>prefixsort_max_string_prefix_length</p></td>
<td><p>integer</p></td>
<td><p>16</p></td>
<td><p>Byte length of the string prefix stored in the prefix-sort buffer. This doesn’t include the null byte.</p></td>
</tr>
<tr class="row-even"><td><p>shuffle_compression_codec</p></td>
<td><p>string</p></td>
<td><p>none</p></td>
<td><p>Specifies the compression algorithm type to compress the shuffle data to
trade CPU for network IO efficiency. The supported compression codecs
are: zlib, snappy, lzo, zstd, lz4 and gzip. none means no compression.</p></td>
</tr>
<tr class="row-odd"><td><p>throw_exception_on_duplicate_map_keys</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>By default, if a key is found in multiple given maps, that key’s value in the resulting map comes from the last one of those maps.
If true, throws exception when duplicate keys are found. This configuration is needed by Spark functions <cite>CreateMap</cite>, <cite>MapFromArrays</cite>, <cite>MapFromEntries</cite>, <cite>StringToMap</cite>, <cite>MapConcat</cite>, <cite>TransformKeys</cite>.</p></td>
</tr>
<tr class="row-even"><td><p>index_lookup_join_max_prefetch_batches</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Specifies the max number of input batches to prefetch to do index lookup ahead. If it is zero,
then process one input batch at a time.</p></td>
</tr>
<tr class="row-odd"><td><p>index_lookup_join_split_output</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If this is true, then the index join operator might split output for each input batch based
on the output batch size control. Otherwise, it tries to produce a single output for each input
batch.</p></td>
</tr>
<tr class="row-even"><td><p>unnest_split_output_batch</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If this is true, then the unnest operator might split output for each input batch based on the
output batch size control. Otherwise, it produces a single output for each input batch.</p></td>
</tr>
<tr class="row-odd"><td><p>max_num_splits_listened_to</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Specifies The max number of input splits to listen to by SplitListener per table scan node per
worker. It’s up to the SplitListener implementation to respect this config.</p></td>
</tr>
<tr class="row-even"><td><p>operator_track_expression_stats</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If this is true, then operators that evaluate expressions will track stats for expressions that
are not special forms and return them as part of their operator stats. Tracking these stats can
be expensive (especially if operator stats are retrieved frequently) and this allows the user to
explicitly enable it.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="expression-evaluation-configuration">
<h2>Expression Evaluation Configuration<a class="headerlink" href="#expression-evaluation-configuration" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>expression.eval_simplified</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>Whether to use the simplified expression evaluation path.</p></td>
</tr>
<tr class="row-odd"><td><p>expression.track_cpu_usage</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>Whether to track CPU usage for individual expressions (supported by call and cast expressions). Can be expensive
when processing small batches, e.g. &lt; 10K rows.</p></td>
</tr>
<tr class="row-even"><td><p>expression.track_cpu_usage_for_functions</p></td>
<td><p>string</p></td>
<td><p>“”</p></td>
<td><p>Comma-separated list of function names to selectively track CPU usage for. Only applicable when
<code class="docutils literal notranslate"><span class="pre">expression.track_cpu_usage</span></code> is set to false. Function names are case-insensitive and will be normalized
to lowercase. This allows fine-grained control over CPU tracking overhead when only specific functions need to
be monitored.</p></td>
</tr>
<tr class="row-odd"><td><p>legacy_cast</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Enables legacy CAST semantics if set to true. CAST(timestamp AS varchar) uses ‘T’ as separator between date and
time (instead of a space), and the year part is not padded.</p></td>
</tr>
<tr class="row-even"><td><p>cast_match_struct_by_name</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>This flag makes the Row conversion to by applied in a way that the casting row field are matched by name instead of position.</p></td>
</tr>
<tr class="row-odd"><td><p>expression.max_array_size_in_reduce</p></td>
<td><p>integer</p></td>
<td><p>100000</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Reduce</span></code> function will throw an error if encountered an array of size greater than this.</p></td>
</tr>
<tr class="row-even"><td><p>expression.max_compiled_regexes</p></td>
<td><p>integer</p></td>
<td><p>100</p></td>
<td><p>Controls maximum number of compiled regular expression patterns per batch.</p></td>
</tr>
<tr class="row-odd"><td><p>debug_disable_expression_with_peeling</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Disable optimization in expression evaluation to peel common dictionary layer from inputs. Should only be used for debugging.</p></td>
</tr>
<tr class="row-even"><td><p>debug_disable_common_sub_expressions</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Disable optimization in expression evaluation to re-use cached results for common sub-expressions. Should only be used for debugging.</p></td>
</tr>
<tr class="row-odd"><td><p>debug_disable_expression_with_memoization</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Disable optimization in expression evaluation to re-use cached results between subsequent input batches that are dictionary encoded and have the same alphabet(underlying flat vector). Should only be used for debugging.</p></td>
</tr>
<tr class="row-even"><td><p>debug_disable_expression_with_lazy_inputs</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Disable optimization in expression evaluation to delay loading of lazy inputs unless required. Should only be used for debugging.</p></td>
</tr>
<tr class="row-odd"><td><p>debug_lambda_function_evaluation_batch_size</p></td>
<td><p>integer</p></td>
<td><p>10000</p></td>
<td><p>Some lambda functions over arrays and maps are evaluated in batches of the underlying elements that comprise the arrays/maps. This is done to make the batch size managable as array vectors can have thousands of elements each and hit scaling limits as implementations typically expect BaseVectors to a couple of thousand entries. This lets up tune those batch sizes. Setting this to zero is setting unlimited batch size.</p></td>
</tr>
<tr class="row-even"><td><p>debug_bing_tile_children_max_zoom_shift</p></td>
<td><p>integer</p></td>
<td><p>5</p></td>
<td><p>The UDF <cite>bing_tile_children</cite> generates the children of a Bing tile based on a specified target zoom level. The number of children produced is determined by the difference between the target zoom level and the zoom level of the input tile. This configuration limits the number of children by capping the maximum zoom level difference, with a default value set to 5. This cap is necessary to prevent excessively large array outputs, which can exceed the size limits of the elements vector in the Velox array vector.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="memory-management">
<h2>Memory Management<a class="headerlink" href="#memory-management" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>max_partial_aggregation_memory</p></td>
<td><p>integer</p></td>
<td><p>16MB</p></td>
<td><p>Maximum amount of memory in bytes for partial aggregation results. Increasing this value can result in less
network transfer and lower CPU utilization by allowing more groups to be kept locally before being flushed,
at the cost of additional memory usage.</p></td>
</tr>
<tr class="row-odd"><td><p>max_extended_partial_aggregation_memory</p></td>
<td><p>integer</p></td>
<td><p>16MB</p></td>
<td><p>Maximum amount of memory in bytes for partial aggregation results if cardinality reduction is below
<cite>partial_aggregation_reduction_ratio_threshold</cite>. Every time partial aggregate results size reaches
<cite>max_partial_aggregation_memory</cite> bytes, the results are flushed. If cardinality reduction is below
<cite>partial_aggregation_reduction_ratio_threshold</cite>,
i.e. <cite>number of result rows / number of input rows &gt; partial_aggregation_reduction_ratio_threshold</cite>,
memory limit for partial aggregation is automatically doubled up to <cite>max_extended_partial_aggregation_memory</cite>.
This adaptation is disabled by default, since the value of <cite>max_extended_partial_aggregation_memory</cite> equals the
value of <cite>max_partial_aggregation_memory</cite>. Specify higher value for <cite>max_extended_partial_aggregation_memory</cite> to enable.</p></td>
</tr>
<tr class="row-even"><td><p>query_memory_reclaimer_priority</p></td>
<td><p>integer</p></td>
<td><p>2147483647</p></td>
<td><p>Priority of the query in the memory pool reclaimer. Lower value means higher priority. This is used in
global arbitration victim selection.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="spilling">
<h2>Spilling<a class="headerlink" href="#spilling" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>Spill memory to disk to avoid exceeding memory limits for the query.</p></td>
</tr>
<tr class="row-odd"><td><p>aggregation_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether HashAggregation operator can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-even"><td><p>join_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether HashBuild and HashProbe operators can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-odd"><td><p>local_merge_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether LocalMerge operators can spill to disk to cap memory usage.</p></td>
</tr>
<tr class="row-even"><td><p>mixed_grouped_mode_hash_join_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>When both <cite>spill_enabled</cite> and <cite>join_spill_enabled</cite> are true, determines if HashProbe and HashBuild are able to spill under mixed grouped execution mode.</p></td>
</tr>
<tr class="row-odd"><td><p>order_by_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether OrderBy operator can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-even"><td><p>window_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether Window operator can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-odd"><td><p>window_spill_min_read_batch_rows</p></td>
<td><p>integer</p></td>
<td><p>1000</p></td>
<td><p>When processing spilled window data, read batches of whole partitions having at least that many rows. Set to 1 to
read one whole partition at a time. Each driver processing the Window operator will process that much data at
once.</p></td>
</tr>
<tr class="row-even"><td><p>row_number_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether RowNumber operator can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-odd"><td><p>topn_row_number_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>spill_enabled</cite> is true, determines whether TopNRowNumber operator can spill to disk under memory pressure.</p></td>
</tr>
<tr class="row-even"><td><p>writer_spill_enabled</p></td>
<td><p>boolean</p></td>
<td><p>true</p></td>
<td><p>When <cite>writer_spill_enabled</cite> is true, determines whether TableWriter operator can flush the buffered data to disk
under memory pressure.</p></td>
</tr>
<tr class="row-odd"><td><p>aggregation_spill_memory_threshold</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Maximum amount of memory in bytes that a final aggregation can use before spilling. 0 means unlimited.</p></td>
</tr>
<tr class="row-even"><td><p>join_spill_memory_threshold</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Maximum amount of memory in bytes that a hash join build side can use before spilling. 0 means unlimited.</p></td>
</tr>
<tr class="row-odd"><td><p>order_by_spill_memory_threshold</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Maximum amount of memory in bytes that an order by can use before spilling. 0 means unlimited.</p></td>
</tr>
<tr class="row-even"><td><p>writer_flush_threshold_bytes</p></td>
<td><p>integer</p></td>
<td><p>96MB</p></td>
<td><p>Minimum memory footprint size required to reclaim memory from a file writer by flushing its buffered data to disk.</p></td>
</tr>
<tr class="row-odd"><td><p>min_spillable_reservation_pct</p></td>
<td><p>integer</p></td>
<td><p>5</p></td>
<td><p>The minimal available spillable memory reservation in percentage of the current memory usage. Suppose the current
memory usage size of M, available memory reservation size of N and min reservation percentage of P,
if M * P / 100 &gt; N, then spiller operator needs to grow the memory reservation with percentage of
‘spillable_reservation_growth_pct’ (see below). This ensures we have sufficient amount of memory reservation to
process the large input outlier.</p></td>
</tr>
<tr class="row-even"><td><p>spillable_reservation_growth_pct</p></td>
<td><p>integer</p></td>
<td><p>10</p></td>
<td><p>The spillable memory reservation growth percentage of the current memory usage. Suppose a growth percentage of N
and the current memory usage size of M, the next memory reservation size will be M * (1 + N / 100). After growing
the memory reservation K times, the memory reservation size will be M * (1 + N / 100) ^ K. Hence the memory
reservation grows along a series of powers of (1 + N / 100). If the memory reservation fails, it starts spilling.</p></td>
</tr>
<tr class="row-odd"><td><p>max_spill_level</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>The maximum allowed spilling level with zero being the initial spilling level. Applies to hash join build
spilling which might use recursive spilling when the build table is very large. -1 means unlimited.
In this case an extremely large query might run out of spilling partition bits. The max spill level
can be used to prevent a query from using too much io and cpu resources.</p></td>
</tr>
<tr class="row-even"><td><p>max_spill_run_rows</p></td>
<td><p>integer</p></td>
<td><p>12582912</p></td>
<td><p>The max number of rows to fill and spill for each spill run. This is used to cap the memory used for spilling.
If it is zero, then there is no limit and spilling might run out of memory. Based on offline test results, the
default value is set to 12 million rows which uses <code class="docutils literal notranslate"><span class="pre">~128MB</span></code> memory when to fill a spill run.
Relation between spill rows and memory usage are as follows:
12 million rows: <code class="docutils literal notranslate"><span class="pre">128</span> <span class="pre">MB</span></code>, 30 million rows: <code class="docutils literal notranslate"><span class="pre">256</span> <span class="pre">MB</span></code>, 60 million rows: <code class="docutils literal notranslate"><span class="pre">512</span> <span class="pre">MB</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>max_spill_file_size</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>The maximum allowed spill file size. Zero means unlimited.</p></td>
</tr>
<tr class="row-even"><td><p>max_spill_bytes</p></td>
<td><p>integer</p></td>
<td><p>107374182400</p></td>
<td><p>The max spill bytes limit set for each query. This is used to cap the storage used for spilling.
If it is zero, then there is no limit and spilling might exhaust the storage or takes too long to run.
The default value is set to 100 GB.</p></td>
</tr>
<tr class="row-odd"><td><p>spill_write_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>4MB</p></td>
<td><p>The maximum size in bytes to buffer the serialized spill data before write to disk for IO efficiency.
If set to zero, buffering is disabled.</p></td>
</tr>
<tr class="row-even"><td><p>spill_read_buffer_size</p></td>
<td><p>integer</p></td>
<td><p>1MB</p></td>
<td><p>The buffer size in bytes to read from one spilled file. If the underlying filesystem supports async
read, we do read-ahead with double buffering, which doubles the buffer used to read from each spill file.</p></td>
</tr>
<tr class="row-odd"><td><p>min_spill_run_size</p></td>
<td><p>integer</p></td>
<td><p>256MB</p></td>
<td><p>The minimum spill run size (bytes) limit used to select partitions for spilling. The spiller tries to spill a
previously spilled partitions if its data size exceeds this limit, otherwise it spills the partition with most data.
If the limit is zero, then the spiller always spills a previously spilled partition if it has any data. This is
to avoid spill from a partition with a small amount of data which might result in generating too many small
spilled files.</p></td>
</tr>
<tr class="row-even"><td><p>spill_compression_codec</p></td>
<td><p>string</p></td>
<td><p>none</p></td>
<td><p>Specifies the compression algorithm type to compress the spilled data before write to disk to trade CPU for IO
efficiency. The supported compression codecs are: zlib, snappy, lzo, zstd, lz4 and gzip.
none means no compression.</p></td>
</tr>
<tr class="row-odd"><td><p>spill_num_max_merge_files</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>The max number of files to merge at a time when merging sorted files into a single ordered stream. 0 means unlimited.
This is used to reduce memory pressure by capping the number of open files when merging spilled sorted files to
avoid using too much memory and causing OOM. Note that this is only applicable for ordered spill.</p></td>
</tr>
<tr class="row-even"><td><p>spill_prefixsort_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Enable the prefix sort or fallback to timsort in spill. The prefix sort is faster than std::sort but requires the
memory to build normalized prefix keys, which might have potential risk of running out of server memory.</p></td>
</tr>
<tr class="row-odd"><td><p>spiller_start_partition_bit</p></td>
<td><p>integer</p></td>
<td><p>29</p></td>
<td><p>The start partition bit which is used with <cite>spiller_num_partition_bits</cite> together to calculate the spilling partition number.</p></td>
</tr>
<tr class="row-even"><td><p>spiller_num_partition_bits</p></td>
<td><p>integer</p></td>
<td><p>3</p></td>
<td><p>The number of bits (N) used to calculate the spilling partition number for hash join and RowNumber: 2 ^ N. At the moment the maximum
value is 3, meaning we only support up to 8-way spill partitioning.ing.</p></td>
</tr>
<tr class="row-odd"><td><p>testing.spill_pct</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Percentage of aggregation or join input batches that will be forced to spill for testing. 0 means no extra spilling.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="aggregation">
<h2>Aggregation<a class="headerlink" href="#aggregation" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>abandon_partial_aggregation_min_rows</p></td>
<td><p>integer</p></td>
<td><p>100,000</p></td>
<td><p>Number of input rows to receive before starting to check whether to abandon partial aggregation.</p></td>
</tr>
<tr class="row-odd"><td><p>abandon_partial_aggregation_min_pct</p></td>
<td><p>integer</p></td>
<td><p>80</p></td>
<td><p>Abandons partial aggregation if number of groups equals or exceeds this percentage of the number of input rows.</p></td>
</tr>
<tr class="row-even"><td><p>aggregation_compaction_bytes_threshold</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Memory threshold in bytes for triggering string compaction during global
aggregation. When total string storage exceeds this limit with high unused
memory ratio, compaction is triggered to reclaim dead strings. Disabled by
default (0). Currently only applies to approx_most_frequent aggregate with
StringView type during global aggregation.</p></td>
</tr>
<tr class="row-odd"><td><p>aggregation_compaction_unused_memory_ratio</p></td>
<td><p>double</p></td>
<td><p>0.25</p></td>
<td><p>Ratio of unused (evicted) bytes to total bytes that triggers compaction.
The value is in the range of [0, 1). Currently only applies to approx_most_frequent
aggregate with StringView type during global aggregation. May be extended
to other aggregation types on-demand.</p></td>
</tr>
<tr class="row-even"><td><p>streaming_aggregation_min_output_batch_rows</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>In streaming aggregation, wait until we have enough number of output rows
to produce a batch of size specified by this. If set to 0, then
Operator::outputBatchRows will be used as the min output batch rows.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="table-scan">
<h2>Table Scan<a class="headerlink" href="#table-scan" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>max_split_preload_per_driver</p></td>
<td><p>integer</p></td>
<td><p>2</p></td>
<td><p>Maximum number of splits to preload per driver. Set to 0 to disable preloading.</p></td>
</tr>
<tr class="row-odd"><td><p>table_scan_scaled_processing_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, enables the scaled table scan processing. For each table scan
plan node, a scan controller is used to control the number of running scan
threads based on the query memory usage. It keeps increasing the number of
running threads until the query memory usage exceeds the threshold defined
by ‘table_scan_scale_up_memory_usage_ratio’.</p></td>
</tr>
<tr class="row-even"><td><p>table_scan_scale_up_memory_usage_ratio</p></td>
<td><p>double</p></td>
<td><p>0.5</p></td>
<td><p>The query memory usage ratio used by scan controller to decide if it can
increase the number of running scan threads. When the query memory usage
is below this ratio, the scan controller scale up the scan processing by
increasing the number of running scan threads, and stop once exceeds this
ratio. The value is in the range of [0, 1]. This only applies if
‘table_scan_scaled_processing_enabled’ is true.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="table-writer">
<h2>Table Writer<a class="headerlink" href="#table-writer" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>task_writer_count</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>The number of parallel table writer threads per task.</p></td>
</tr>
<tr class="row-odd"><td><p>task_partitioned_writer_count</p></td>
<td><p>integer</p></td>
<td><p>task_writer_count</p></td>
<td><p>The number of parallel table writer threads per task for partitioned
table writes. If not set, use ‘task_writer_count’ as default.</p></td>
</tr>
<tr class="row-even"><td><p>scaled_writer_rebalance_max_memory_usage_ratio</p></td>
<td><p>double</p></td>
<td><p>0.7</p></td>
<td><p>The max ratio of a query used memory to its max capacity, and the scale
writer exchange stops scaling writer processing if the query’s current
memory usage exceeds this ratio. The value is in the range of (0, 1].</p></td>
</tr>
<tr class="row-odd"><td><p>scaled_writer_max_partitions_per_writer</p></td>
<td><p>integer</p></td>
<td><p>128</p></td>
<td><p>The max number of logical table partitions that can be assigned to a
single table writer thread. The logical table partition is used by local
exchange writer for writer scaling, and multiple physical table
partitions can be mapped to the same logical table partition based on the
hash value of calculated partitioned ids.</p></td>
</tr>
<tr class="row-even"><td><p>scaled_writer_min_partition_processed_bytes_rebalance_threshold</p></td>
<td><p>integer</p></td>
<td><p>128MB</p></td>
<td><p>Minimum amount of data processed by a logical table partition to trigger
writer scaling if it is detected as overloaded by scale wrirer exchange.</p></td>
</tr>
<tr class="row-odd"><td><p>scaled_writer_min_processed_bytes_rebalance_threshold</p></td>
<td><p>integer</p></td>
<td><p>256MB</p></td>
<td><p>Minimum amount of data processed by all the logical table partitions to
trigger skewed partition rebalancing by scale writer exchange.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="connector-config">
<h2>Connector Config<a class="headerlink" href="#connector-config" title="Link to this heading">¶</a></h2>
<p>Connector config is initialized on velox runtime startup and is shared among queries as the default config across all connectors.
Each query can override the config by setting corresponding query session properties such as in Prestissimo.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.4%" />
<col style="width: 15.4%" />
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 53.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>user</p></th>
<th class="head"></th>
<th class="head"><p>string</p></th>
<th class="head"><p>“”</p></th>
<th class="head"><p>The user of the query. Used for storage logging.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>source</p></td>
<td></td>
<td><p>string</p></td>
<td><p>“”</p></td>
<td><p>The source of the query. Used for storage access and logging.</p></td>
</tr>
<tr class="row-odd"><td><p>schema</p></td>
<td></td>
<td><p>string</p></td>
<td><p>“”</p></td>
<td><p>The schema of the query. Used for storage logging.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="hive-connector">
<h2>Hive Connector<a class="headerlink" href="#hive-connector" title="Link to this heading">¶</a></h2>
<p>Hive Connector config is initialized on velox runtime startup and is shared among queries as the default config.
Each query can override the config by setting corresponding query session properties such as in Prestissimo.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.4%" />
<col style="width: 15.4%" />
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 53.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Property Name</p></th>
<th class="head"><p>Session Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hive.max-partitions-per-writers</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>100</p></td>
<td><p>Maximum number of (bucketed) partitions per a single table writer instance.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.max-bucket-count</p></td>
<td><p>hive.max_bucket_count</p></td>
<td><p>integer</p></td>
<td><p>100000</p></td>
<td><p>Maximum number of buckets that a table writer is allowed to write to.</p></td>
</tr>
<tr class="row-even"><td><p>insert-existing-partitions-behavior</p></td>
<td><p>insert_existing_partitions_behavior</p></td>
<td><p>string</p></td>
<td><p>ERROR</p></td>
<td><p><strong>Allowed values:</strong> <code class="docutils literal notranslate"><span class="pre">OVERWRITE</span></code>, <code class="docutils literal notranslate"><span class="pre">ERROR</span></code>. The behavior on insert existing partitions. This property only derives
the update mode field of the table writer operator output. <code class="docutils literal notranslate"><span class="pre">OVERWRITE</span></code>
sets the update mode to indicate overwriting a partition if exists. <code class="docutils literal notranslate"><span class="pre">ERROR</span></code> sets the update mode to indicate
error throwing if writing to an existing partition.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.immutable-partitions</p></td>
<td></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>True if appending data to an existing unpartitioned table is allowed. Currently this configuration does not
support appending to existing partitions.</p></td>
</tr>
<tr class="row-even"><td><p>file-column-names-read-as-lower-case</p></td>
<td></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>True if reading the source file column names as lower case, and planner should guarantee
the input column name and filter is also lower case to achive case-insensitive read.</p></td>
</tr>
<tr class="row-odd"><td><p>partition_path_as_lower_case</p></td>
<td></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, the partition directory will be converted to lowercase when executing a table write operation.</p></td>
</tr>
<tr class="row-even"><td><p>allow-null-partition-keys</p></td>
<td><p>allow_null_partition_keys</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Determines whether null values for partition keys are allowed or not. If not, fails with “Partition key must
not be null” error message when writing data with null partition key.
Null check for partitioning key should be used only when partitions are generated dynamically during query execution.
For queries that write to fixed partitions, this check should happen much earlier before the Velox execution even starts.</p></td>
</tr>
<tr class="row-odd"><td><p>ignore_missing_files</p></td>
<td></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, splits that refer to missing files don’t generate errors and are processed as empty splits.</p></td>
</tr>
<tr class="row-even"><td><p>max-coalesced-bytes</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>128MB</p></td>
<td><p>Maximum size in bytes to coalesce requests to be fetched in a single request.</p></td>
</tr>
<tr class="row-odd"><td><p>max-coalesced-distance</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>512KB</p></td>
<td><p>Maximum distance in capacity units between chunks to be fetched that may be coalesced into a single request.</p></td>
</tr>
<tr class="row-even"><td><p>load-quantum</p></td>
<td><p>load-quantum</p></td>
<td><p>integer</p></td>
<td><p>8MB</p></td>
<td><p>Define the size of each coalesce load request. E.g. in Parquet scan, if it’s bigger than rowgroup size then the whole row group can be fetched together. Otherwise, the row group will be fetched column chunk by column chunk</p></td>
</tr>
<tr class="row-odd"><td><p>num-cached-file-handles</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>20000</p></td>
<td><p>Maximum number of entries in the file handle cache. The value must be non-negative. Zero value
indicates infinite cache capacity.</p></td>
</tr>
<tr class="row-even"><td><p>file-handle-cache-enabled</p></td>
<td></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Enables caching of file handles if true. Disables caching if false. File handle cache should be
disabled if files are not immutable, i.e. file content may change while file path stays the same.</p></td>
</tr>
<tr class="row-odd"><td><p>sort-writer-max-output-rows</p></td>
<td><p>sort_writer_max_output_rows</p></td>
<td><p>integer</p></td>
<td><p>1024</p></td>
<td><p>Maximum number of rows for sort writer in one batch of output. This is to limit the memory usage of sort writer.</p></td>
</tr>
<tr class="row-even"><td><p>sort-writer-max-output-bytes</p></td>
<td><p>sort_writer_max_output_bytes</p></td>
<td><p>string</p></td>
<td><p>10MB</p></td>
<td><p>Maximum bytes for sort writer in one batch of output. This is to limit the memory usage of sort writer.</p></td>
</tr>
<tr class="row-odd"><td><p>file-preload-threshold</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>8MB</p></td>
<td><p>Usually Velox fetches the meta data firstly then fetch the rest of file. But if the file is very small, Velox can fetch the whole file directly to avoid multiple IO requests.
The parameter controls the threshold when whole file is fetched.</p></td>
</tr>
<tr class="row-even"><td><p>footer-estimated-size</p></td>
<td></td>
<td><p>integer</p></td>
<td><p>1MB</p></td>
<td><p>Define the estimation of footer size in ORC and Parquet format. The footer data includes version, schema, and meta data for every columns which may or may not need to be fetched later.
The parameter controls the size when footer is fetched each time. Bigger value can decrease the IO requests but may fetch more useless meta data.</p></td>
</tr>
<tr class="row-odd"><td><p>cache.no_retention</p></td>
<td><p>cache.no_retention</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, evict out a query scanned data out of in-memory cache right after the access,
and also skip staging to the ssd cache. This helps to prevent the cache space pollution
from the one-time table scan by large batch query when mixed running with interactive
query which has high data locality.</p></td>
</tr>
<tr class="row-even"><td><p>hive.reader.stats_based_filter_reorder_disabaled</p></td>
<td><p>hive.reader.stats_based_filter_reorder_disabaled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, disable the stats based filter reordering during the read processing, and the
filter execution order is totally determined by the filter type. Otherwise, the file
reader will dynamically adjust the filter execution order based on the past filter
execution stats.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.reader.timestamp-partition-value-as-local-time</p></td>
<td><p>hive.reader.timestamp_partition_value_as_local_time</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Reads timestamp partition value as local time if true. Otherwise, reads as UTC.</p></td>
</tr>
<tr class="row-even"><td><p>hive.preserve-flat-maps-in-memory</p></td>
<td><p>hive.preserve_flat_maps_in_memory</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Whether to preserve flat maps in memory as FlatMapVectors instead of converting them to MapVectors. This is only applied during data reading inside the DWRF and Nimble readers, not during downstream processing like expression evaluation etc.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.max-rows-per-index-request</p></td>
<td><p>hive.max_rows_per_index_request</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>Maximum number of output rows to return per index lookup request. The limit is applied to the actual output rows
after filtering. 0 means no limit (default).</p></td>
</tr>
</tbody>
</table>
<section id="orc-file-format-configuration">
<h3><code class="docutils literal notranslate"><span class="pre">ORC</span> <span class="pre">File</span> <span class="pre">Format</span> <span class="pre">Configuration</span></code><a class="headerlink" href="#orc-file-format-configuration" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.4%" />
<col style="width: 15.4%" />
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 53.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Property Name</p></th>
<th class="head"><p>Session Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hive.orc.writer.stripe-max-size</p></td>
<td><p>orc_optimized_writer_max_stripe_size</p></td>
<td><p>string</p></td>
<td><p>64M</p></td>
<td><p>Maximum stripe size in orc writer.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.orc.writer.dictionary-max-memory</p></td>
<td><p>orc_optimized_writer_max_dictionary_memory</p></td>
<td><p>string</p></td>
<td><p>16M</p></td>
<td><p>Maximum dictionary memory that can be used in orc writer.</p></td>
</tr>
<tr class="row-even"><td><p>hive.orc.writer.integer-dictionary-encoding-enabled</p></td>
<td><p>orc_optimized_writer_integer_dictionary_encoding_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether or not dictionary encoding of integer types should be used by the ORC writer.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.orc.writer.string-dictionary-encoding-enabled</p></td>
<td><p>orc_optimized_writer_string_dictionary_encoding_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether or not dictionary encoding of string types should be used by the ORC writer.</p></td>
</tr>
<tr class="row-even"><td><p>hive.orc.writer.linear-stripe-size-heuristics</p></td>
<td><p>orc_writer_linear_stripe_size_heuristics</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Enables historical based stripe size estimation after compression.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.orc.writer.min-compression-size</p></td>
<td><p>orc_writer_min_compression_size</p></td>
<td><p>integer</p></td>
<td><p>1024</p></td>
<td><p>Minimal number of items in an encoded stream.</p></td>
</tr>
<tr class="row-even"><td><p>hive.orc.writer.compression-level</p></td>
<td><p>orc_optimized_writer_compression_level</p></td>
<td><p>tinyint</p></td>
<td><p>3 for ZSTD and 4 for ZLIB</p></td>
<td><p>The compression level to use with ZLIB and ZSTD.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="parquet-file-format-configuration">
<h3><code class="docutils literal notranslate"><span class="pre">Parquet</span> <span class="pre">File</span> <span class="pre">Format</span> <span class="pre">Configuration</span></code><a class="headerlink" href="#parquet-file-format-configuration" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 15.4%" />
<col style="width: 15.4%" />
<col style="width: 7.7%" />
<col style="width: 7.7%" />
<col style="width: 53.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Configuration Property Name</p></th>
<th class="head"><p>Session Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hive.parquet.writer.enable-dictionary</p></td>
<td><p>hive.parquet.writer.enable_dictionary</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Whether to enable dictionary encoding when writing into Parquet through the Arrow bridge.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.parquet.writer.dictionary-page-size-limit</p></td>
<td><p>hive.parquet.writer.dictionary_page_size_limit</p></td>
<td><p>string</p></td>
<td><p>1MB</p></td>
<td><p>Dictionary Page size used when writing into Parquet through Arrow bridge. This setting is applicable only when dictionary encoding is enabled.</p></td>
</tr>
<tr class="row-even"><td><p>hive.parquet.writer.timestamp-unit</p></td>
<td><p>hive.parquet.writer.timestamp_unit</p></td>
<td><p>tinyint</p></td>
<td><p>9</p></td>
<td><p>Timestamp unit used when writing timestamps into Parquet through Arrow bridge.
Valid values are 3 (millisecond), 6 (microsecond), and 9 (nanosecond).</p></td>
</tr>
<tr class="row-odd"><td><p>hive.parquet.writer.datapage-version</p></td>
<td><p>hive.parquet.writer.datapage_version</p></td>
<td><p>string</p></td>
<td><p>V1</p></td>
<td><p>Data Page version used when writing into Parquet through Arrow bridge.
Valid values are “V1” and “V2”.</p></td>
</tr>
<tr class="row-even"><td><p>hive.parquet.writer.page-size</p></td>
<td><p>hive.parquet.writer.page_size</p></td>
<td><p>string</p></td>
<td><p>1MB</p></td>
<td><p>Data Page size used when writing into Parquet through Arrow bridge.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.parquet.writer.batch-size</p></td>
<td><p>hive.parquet.writer.batch_size</p></td>
<td><p>integer</p></td>
<td><p>1024</p></td>
<td><p>Batch size used when writing into Parquet through Arrow bridge.</p></td>
</tr>
<tr class="row-even"><td><p>hive.parquet.writer.created-by</p></td>
<td></td>
<td><p>string</p></td>
<td><p>parquet-cpp-velox version 0.0.0</p></td>
<td><p>Created-by value used when writing to Parquet.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="amazon-s3-configuration">
<h3><code class="docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">S3</span> <span class="pre">Configuration</span></code><a class="headerlink" href="#amazon-s3-configuration" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 8.3%" />
<col style="width: 8.3%" />
<col style="width: 58.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hive.s3.use-instance-credentials</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Use the EC2 metadata service to retrieve API credentials. This works with IAM roles in EC2.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.aws-access-key</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Default AWS access key to use.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.aws-secret-key</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Default AWS secret key to use.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.endpoint</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The S3 storage endpoint server. This can be used to connect to an S3-compatible storage system instead of AWS.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.endpoint.region</p></td>
<td><p>string</p></td>
<td><p>us-east-1</p></td>
<td><p>The S3 storage endpoint server region. Default is set by the AWS SDK. If not configured, region will be attempted
to be parsed from the hive.s3.endpoint value.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.path-style-access</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Use path-style access for all requests to the S3-compatible storage. This is for S3-compatible storage that
doesn’t support virtual-hosted-style access.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.ssl.enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>Use HTTPS to communicate with the S3 API.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.log-level</p></td>
<td><p>string</p></td>
<td><p>FATAL</p></td>
<td><p><strong>Allowed values:</strong> “OFF”, “FATAL”, “ERROR”, “WARN”, “INFO”, “DEBUG”, “TRACE”.
Granularity of logging generated by the AWS C++ SDK library.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.log-location</p></td>
<td><p>string</p></td>
<td><p>“”</p></td>
<td><p>Specifies the path where the log files are created. Generated log files start with “aws_sdk_” and use the default AWS S3 logger. Example: setting “/tmp” results in files “/tmp/aws_sdk_*”.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.payload-signing-policy</p></td>
<td><p>string</p></td>
<td><p>Never</p></td>
<td><p><strong>Allowed values:</strong> “Always”, “RequestDependent”, “Never”.
When set to “Always”, the payload checksum is included in the signature calculation.
When set to “RequestDependent”, the payload checksum is included based on the value returned by “AmazonWebServiceRequest::SignBody()”.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.iam-role</p></td>
<td><p>string</p></td>
<td></td>
<td><p>IAM role to assume.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.iam-role-session-name</p></td>
<td><p>string</p></td>
<td><p>velox-session</p></td>
<td><p>Session name associated with the IAM role.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.use-proxy-from-env</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>Utilize the configuration of the environment variables http_proxy, https_proxy, and no_proxy for use with the S3 API.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.connect-timeout</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Socket connect timeout.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.socket-timeout</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Socket read timeout.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.max-connections</p></td>
<td><p>integer</p></td>
<td></td>
<td><p>Maximum concurrent TCP connections for a single http client.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.max-attempts</p></td>
<td><p>integer</p></td>
<td></td>
<td><p>Maximum attempts for connections to a single http client, work together with retry-mode. By default, it’s 3 for standard/adaptive mode
and 10 for legacy mode.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.retry-mode</p></td>
<td><p>string</p></td>
<td></td>
<td><p><strong>Allowed values:</strong> “standard”, “adaptive”, “legacy”. By default it’s empty, S3 client will be created with RetryStrategy.
Legacy mode only enables throttled retry for transient errors.
Standard mode is built on top of legacy mode and has throttled retry enabled for throttling errors apart from transient errors.
Adaptive retry mode dynamically limits the rate of AWS requests to maximize success rate.</p></td>
</tr>
<tr class="row-even"><td><p>hive.s3.aws-credentials-provider</p></td>
<td><p>string</p></td>
<td></td>
<td><p>A custom credential provider, if specified, will be used to create the client in favor of other authentication mechanisms.
The provider must be registered using “registerAWSCredentialsProvider” before it can be used.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.s3.aws-imds-enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>AWS Instance Metadata Service (IMDS) is an AWS EC2 instance component used by applications to securely access metadata.
We must disable it on other instances to avoid high first-time read latency from S3 compatible object storages.</p></td>
</tr>
</tbody>
</table>
<section id="bucket-level-configuration">
<h4>Bucket Level Configuration<a class="headerlink" href="#bucket-level-configuration" title="Link to this heading">¶</a></h4>
<p>All “hive.s3.*” config (except “hive.s3.log-level”) can be set on a per-bucket basis. The bucket-specific option is set by
replacing the “hive.s3.” prefix on a config with “hive.s3.bucket.BUCKETNAME.”, where BUCKETNAME is the name of the
bucket. e.g. the endpoint for a bucket named “velox” can be specified by the config “hive.s3.bucket.velox.endpoint”.
When connecting to a bucket, all options explicitly set will override the base “hive.s3.” values.
These semantics are similar to the <a class="reference external" href="https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html">Apache Hadoop-Aws module</a>.</p>
</section>
</section>
<section id="google-cloud-storage-configuration">
<h3><code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Cloud</span> <span class="pre">Storage</span> <span class="pre">Configuration</span></code><a class="headerlink" href="#google-cloud-storage-configuration" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 27.3%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 54.5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>hive.gcs.endpoint</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The GCS storage URI.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.gcs.json-key-file-path</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The GCS service account configuration JSON key file.</p></td>
</tr>
<tr class="row-even"><td><p>hive.gcs.max-retry-count</p></td>
<td><p>integer</p></td>
<td></td>
<td><p>The GCS maximum retry counter of transient errors.</p></td>
</tr>
<tr class="row-odd"><td><p>hive.gcs.max-retry-time</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The GCS maximum time allowed to retry transient errors.</p></td>
</tr>
<tr class="row-even"><td><p>hive.gcs.auth.access-token-provider</p></td>
<td><p>string</p></td>
<td></td>
<td><p>A custom OAuth credential provider, if specified, will be used to create the client in favor of other
authentication mechanisms.
The provider must be registered using “registerGcsOAuthCredentialsProvider” before it can be used.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="azure-blob-storage-configuration">
<h3><code class="docutils literal notranslate"><span class="pre">Azure</span> <span class="pre">Blob</span> <span class="pre">Storage</span> <span class="pre">Configuration</span></code><a class="headerlink" href="#azure-blob-storage-configuration" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 27.3%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 54.5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fs.azure.account.auth.type.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Specifies the authentication mechanism to use for Azure storage accounts.
<strong>Allowed values:</strong> “SharedKey”, “OAuth”, “SAS”.
“SharedKey”: Uses the storage account name and key for authentication.
“OAuth”: Utilizes OAuth tokens for secure authentication.
“SAS”: Employs Shared Access Signatures for granular access control.
To create Azure clients with the configured authentication type, the caller must
register the corresponding Azure client provider from the configuration by calling
<cite>registerAzureClientProvider</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p>fs.azure.account.key.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The credentials to access the specific Azure Blob Storage account, replace &lt;storage-account&gt; with the name of your Azure Storage account.
This property aligns with how Spark configures Azure account key credentials for accessing Azure storage, by setting this property multiple
times with different storage account names, you can access multiple Azure storage accounts.</p></td>
</tr>
<tr class="row-even"><td><p>fs.azure.sas.fixed.token.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Specifies a fixed SAS (Shared Access Signature) token for accessing Azure storage.
This token provides scoped and time-limited access to specific resources.
Use this property when a pre-generated SAS token is used for authentication.</p></td>
</tr>
<tr class="row-odd"><td><p>fs.azure.account.oauth2.client.id.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Specifies the client ID of the Azure AD application used for OAuth 2.0 authentication.
This client ID is required when using OAuth as the authentication type.</p></td>
</tr>
<tr class="row-even"><td><p>fs.azure.account.oauth2.client.secret.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Specifies the client secret of the Azure AD application used for OAuth 2.0 authentication.
This secret is required in conjunction with the client ID to authenticate the application.</p></td>
</tr>
<tr class="row-odd"><td><p>fs.azure.account.oauth2.client.endpoint.&lt;storage-account&gt;.dfs.core.windows.net</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Specifies the OAuth 2.0 token endpoint URL for the Azure AD application.
This endpoint is used to acquire access tokens for authenticating with Azure storage.
The URL follows the format: <cite>https://login.microsoftonline.com/&lt;tenant-id&gt;/oauth2/token</cite>.</p></td>
</tr>
<tr class="row-even"><td><p>fs.azure.sas.token.renew.period.for.streams</p></td>
<td><p>string</p></td>
<td><p>120</p></td>
<td><p>Specifies the period in seconds to re-use SAS tokens until the expiry is within this number of seconds.
This configuration is used together with <cite>registerSasTokenProvider</cite> for dynamic SAS token renewal.
When a SAS token is close to expiry, it will be renewed by getting a new token from the provider.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="presto-specific-configuration">
<h2>Presto-specific Configuration<a class="headerlink" href="#presto-specific-configuration" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>presto.array_agg.ignore_nulls</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, <code class="docutils literal notranslate"><span class="pre">array_agg</span></code> function ignores null inputs.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="spark-specific-configuration">
<h2>Spark-specific Configuration<a class="headerlink" href="#spark-specific-configuration" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 18.2%" />
<col style="width: 9.1%" />
<col style="width: 9.1%" />
<col style="width: 63.6%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>spark.ansi_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, Spark function’s behavior is ANSI-compliant, e.g. throws runtime exception instead
of returning null on invalid inputs. It affects only functions explicitly marked as “ANSI compliant”.
Note: This feature is still under development to achieve full ANSI compliance. Users can
refer to the Spark function documentation to verify the current support status of a specific
function.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.legacy_size_of_null</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If false, <code class="docutils literal notranslate"><span class="pre">size</span></code> function returns null for null input.</p></td>
</tr>
<tr class="row-even"><td><p>spark.bloom_filter.expected_num_items</p></td>
<td><p>integer</p></td>
<td><p>1000000</p></td>
<td><p>The default number of expected items for the bloom filter in <a class="reference internal" href="functions/spark/aggregate.html#id1" title="bloom_filter_agg"><code class="xref spark spark-func docutils literal notranslate"><span class="pre">bloom_filter_agg()</span></code></a> function.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.bloom_filter.num_bits</p></td>
<td><p>integer</p></td>
<td><p>8388608</p></td>
<td><p>The default number of bits to use for the bloom filter in <a class="reference internal" href="functions/spark/aggregate.html#id1" title="bloom_filter_agg"><code class="xref spark spark-func docutils literal notranslate"><span class="pre">bloom_filter_agg()</span></code></a> function.</p></td>
</tr>
<tr class="row-even"><td><p>spark.bloom_filter.max_num_bits</p></td>
<td><p>integer</p></td>
<td><p>4194304</p></td>
<td><p>The maximum number of bits to use for the bloom filter in <a class="reference internal" href="functions/spark/aggregate.html#id1" title="bloom_filter_agg"><code class="xref spark spark-func docutils literal notranslate"><span class="pre">bloom_filter_agg()</span></code></a> function,
the value of this config can not exceed the default value.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.partition_id</p></td>
<td><p>integer</p></td>
<td></td>
<td><p>The current task’s Spark partition ID. It’s set by the query engine (Spark) prior to task execution.</p></td>
</tr>
<tr class="row-even"><td><p>spark.legacy_date_formatter</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, <a class="reference external" href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">Simple Date Format</a> is used for time formatting and parsing. Joda date formatter is used by default.
Joda date formatter performs strict checking of its input and uses different pattern string.
For example, the 2015-07-22 10:00:00 timestamp cannot be parsed if pattern is yyyy-MM-dd because the parser does not consume whole input.
Another example is that the ‘W’ pattern, which means week in month, is not supported. For more differences, see <a class="reference external" href="https://github.com/facebookincubator/velox/issues/10354">#10354</a>.</p></td>
</tr>
<tr class="row-odd"><td><p>spark.legacy_statistical_aggregate</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, Spark statistical aggregation functions including skewness, kurtosis, stddev, stddev_samp, variance,
var_samp, covar_samp and corr will return NaN instead of NULL when dividing by zero during expression evaluation.</p></td>
</tr>
<tr class="row-even"><td><p>spark.json_ignore_null_fields</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, ignore null fields when generating JSON string. If false, null fields are included with a null value.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="tracing">
<h2>Tracing<a class="headerlink" href="#tracing" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 8.3%" />
<col style="width: 8.3%" />
<col style="width: 58.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>query_trace_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, enable query tracing.</p></td>
</tr>
<tr class="row-odd"><td><p>query_trace_dir</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The root directory to store the tracing data and metadata for a query.</p></td>
</tr>
<tr class="row-even"><td><p>query_trace_node_id</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The plan node id whose input data will be trace. If it is empty, then we only trace the
query metadata which includes the query plan and configs etc.</p></td>
</tr>
<tr class="row-odd"><td><p>query_trace_task_reg_exp</p></td>
<td><p>string</p></td>
<td></td>
<td><p>The regexp of traced task id. We only enable trace on a task if its id matches.</p></td>
</tr>
<tr class="row-even"><td><p>query_trace_max_bytes</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>The max trace bytes limit. Tracing is disabled if zero.</p></td>
</tr>
<tr class="row-odd"><td><p>query_trace_dry_run</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>If true, we only collect the input trace for a given operator but without the actual
execution. This is used for crash debugging.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="cudf-specific-configuration-experimental">
<h2>Cudf-specific Configuration (Experimental)<a class="headerlink" href="#cudf-specific-configuration-experimental" title="Link to this heading">¶</a></h2>
<p>These configurations are available when <a class="reference external" href="https://github.com/facebookincubator/velox/blob/main/velox/experimental/cudf/README.md#getting-started-with-velox-cudf">compiled with cuDF</a>.
Note: These configurations are experimental and subject to change.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 8.3%" />
<col style="width: 8.3%" />
<col style="width: 58.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Property Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>cudf.enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, enable cuDF. By default, it is enabled if compiled with cuDF.</p></td>
</tr>
<tr class="row-odd"><td><p>cudf.memory_resource</p></td>
<td><p>string</p></td>
<td><p>async</p></td>
<td><p>The memory resource to use for cuDF. Possible values are (cuda, pool, async, arena, managed, managed_pool, prefetch_managed, prefetch_managed_pool).
The prefetch options enable automatic prefetching for better GPU memory performance: prefetch_managed uses CUDA unified memory with prefetching,
prefetch_managed_pool uses a pooled version of CUDA unified memory with prefetching.</p></td>
</tr>
<tr class="row-even"><td><p>cudf.memory_percent</p></td>
<td><p>integer</p></td>
<td><p>50</p></td>
<td><p>The initial percent of GPU memory to allocate for pool or arena memory resources.</p></td>
</tr>
<tr class="row-odd"><td><p>cudf.function_name_prefix</p></td>
<td><p>string</p></td>
<td><p>“”</p></td>
<td><p>The prefix to use for the function names in cuDF.</p></td>
</tr>
<tr class="row-even"><td><p>cudf.ast_expression_enabled</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, enable using cuDF AST-based expression evaluation when supported.</p></td>
</tr>
<tr class="row-odd"><td><p>cudf.ast_expression_priority</p></td>
<td><p>integer</p></td>
<td><p>100</p></td>
<td><p>Priority of cuDF AST expressions. Higher value wins when multiple cuDF execution options are available for the same Velox expression. Standalone cuDF functions have priority 50. If enabled, with a default priority of 100, AST will be chosen as replacement for cudf execution.</p></td>
</tr>
<tr class="row-even"><td><p>cudf.allow_cpu_fallback</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, allow falling back to Velox CPU execution when an operation is not supported in cuDF execution. If false, an error will be thrown if an operation is not supported in cuDF execution.</p></td>
</tr>
<tr class="row-odd"><td><p>cudf.debug_enabled</p></td>
<td><p>bool</p></td>
<td><p>false</p></td>
<td><p>If true, enable debug printing.</p></td>
</tr>
<tr class="row-even"><td><p>cudf.log_fallback</p></td>
<td><p>bool</p></td>
<td><p>true</p></td>
<td><p>If true, log a reason for falling back to Velox CPU execution, when an operation is not supported in cuDF execution.</p></td>
</tr>
</tbody>
</table>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Configuration properties</a><ul>
<li><a class="reference internal" href="#generic-configuration">Generic Configuration</a></li>
<li><a class="reference internal" href="#expression-evaluation-configuration">Expression Evaluation Configuration</a></li>
<li><a class="reference internal" href="#memory-management">Memory Management</a></li>
<li><a class="reference internal" href="#spilling">Spilling</a></li>
<li><a class="reference internal" href="#aggregation">Aggregation</a></li>
<li><a class="reference internal" href="#table-scan">Table Scan</a></li>
<li><a class="reference internal" href="#table-writer">Table Writer</a></li>
<li><a class="reference internal" href="#connector-config">Connector Config</a></li>
<li><a class="reference internal" href="#hive-connector">Hive Connector</a><ul>
<li><a class="reference internal" href="#orc-file-format-configuration"><code class="docutils literal notranslate"><span class="pre">ORC</span> <span class="pre">File</span> <span class="pre">Format</span> <span class="pre">Configuration</span></code></a></li>
<li><a class="reference internal" href="#parquet-file-format-configuration"><code class="docutils literal notranslate"><span class="pre">Parquet</span> <span class="pre">File</span> <span class="pre">Format</span> <span class="pre">Configuration</span></code></a></li>
<li><a class="reference internal" href="#amazon-s3-configuration"><code class="docutils literal notranslate"><span class="pre">Amazon</span> <span class="pre">S3</span> <span class="pre">Configuration</span></code></a><ul>
<li><a class="reference internal" href="#bucket-level-configuration">Bucket Level Configuration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#google-cloud-storage-configuration"><code class="docutils literal notranslate"><span class="pre">Google</span> <span class="pre">Cloud</span> <span class="pre">Storage</span> <span class="pre">Configuration</span></code></a></li>
<li><a class="reference internal" href="#azure-blob-storage-configuration"><code class="docutils literal notranslate"><span class="pre">Azure</span> <span class="pre">Blob</span> <span class="pre">Storage</span> <span class="pre">Configuration</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#presto-specific-configuration">Presto-specific Configuration</a></li>
<li><a class="reference internal" href="#spark-specific-configuration">Spark-specific Configuration</a></li>
<li><a class="reference internal" href="#tracing">Tracing</a></li>
<li><a class="reference internal" href="#cudf-specific-configuration-experimental">Cudf-specific Configuration (Experimental)</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="functions/delta/functions.html"
                          title="previous chapter">Delta Lake Functions</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="monitoring.html"
                          title="next chapter">Monitoring</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/configs.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="monitoring.html" title="Monitoring"
             >next</a> |</li>
        <li class="right" >
          <a href="functions/delta/functions.html" title="Delta Lake Functions"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Velox  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Configuration properties</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright TBD.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.4.7.
    </div>
  </body>
</html>